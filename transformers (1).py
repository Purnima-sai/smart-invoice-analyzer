# -*- coding: utf-8 -*-
"""transformers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/102QQMRX47okIDiiKG8SQHGMf5qN5fb8c
"""

!pip install transformers

from transformers import pipeline

analyzer=pipeline("sentiment-analysis")

text_input=input('enter some text:')

result=analyzer(text_input)

result

text_input=input('enter some text:')

result=analyzer(text_input)
result

text_input=input('enter')
result=analyzer(text_input)
result

text_input=input('enter')
result=analyzer(text_input)
result

generate=pipeline('text-generation')

stmt=input('enter some short text:')

res=generate(stmt,max_length=200,num_return_sequences=1)

res

entity_recognition=pipeline('ner')

statement=input('enter some short text:')

result=entity_recognition(statement)

result

qa = pipeline("question-answering")

story = """Once upon a time, there was a brave girl named Asha who lived in a village near the forest.
She loved adventures and helped everyone in need."""
question = "Who is adventurous?"

result = qa(question=question, context=story)
print("Answer:", result['answer'])

SummarizationPipeline = pipeline("summarization")

msg=input('enter statement')

summary = SummarizationPipeline(msg, max_length=50, min_length=20, do_sample=False)
print("Summary:", summary[0]['summary_text'])

# Install Python libraries
!pip install pytesseract pillow spacy transformers torch

# Install spaCy English model
!python -m spacy download en_core_web_sm

# Install Tesseract OCR (slowest step ~2 minutes)
!apt-get install -y tesseract-ocr

import pytesseract
from PIL import Image
import spacy
from transformers import pipeline
import re

pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

from google.colab import files
uploaded = files.upload()

# Step 1: OCR
def extract_text_from_image(image_path):
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image)
    return text

# Step 2: Named Entity Recognition
def extract_entities(text):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(text)

    entities = {
        "ORG": [],        # Organizations (vendors)
        "MONEY": [],      # Prices
        "DATE": [],       # Invoice or due dates
        "PRODUCT": []     # Custom items
    }

    for ent in doc.ents:
        if ent.label_ in entities:
            entities[ent.label_].append(ent.text)

    # Additional: product lines (heuristic)
    lines = text.split('\n')
    product_lines = [line for line in lines if re.search(r'[a-zA-Z]+\s+\d+(\.\d+)?', line)]
    entities["PRODUCT"].extend(product_lines)

    return entities

# Step 3: Summarization
def summarize_text(text):
    summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
    summary = summarizer(text, max_length=50, min_length=20, do_sample=False)
    return summary[0]['summary_text']

# Step 4: Main analysis
def analyze_invoice(image_path):
    print("üîç Extracting text with OCR...")
    text = extract_text_from_image(image_path)

    print("\nüß† Running NER...")
    entities = extract_entities(text)

    print("\nüìÉ Summary:")
    summary = summarize_text(text)
    print(summary)

    print("\nüîé Extracted Entities:")
    for key, val in entities.items():
        print(f"{key}: {set(val)}")  # Unique values only

# Replace with the actual filename you uploaded
analyze_invoice("/content/invoice.webp")

